{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6884747,"sourceType":"datasetVersion","datasetId":3955447}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:30:32.649592Z","iopub.execute_input":"2026-01-04T16:30:32.650390Z","iopub.status.idle":"2026-01-04T16:30:32.655267Z","shell.execute_reply.started":"2026-01-04T16:30:32.650362Z","shell.execute_reply":"2026-01-04T16:30:32.654502Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/ucf101-videos\"\n\nactions = sorted(os.listdir(DATASET_PATH))\nNUM_CLASSES = len(actions)\nlabel_map = {action: idx for idx, action in enumerate(actions)}\n\nprint(\"Number of action classes:\", NUM_CLASSES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:30:44.412270Z","iopub.execute_input":"2026-01-04T16:30:44.413020Z","iopub.status.idle":"2026-01-04T16:30:44.418547Z","shell.execute_reply.started":"2026-01-04T16:30:44.412982Z","shell.execute_reply":"2026-01-04T16:30:44.417951Z"}},"outputs":[{"name":"stdout","text":"Number of action classes: 4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"IMG_SIZE = 224\nFRAMES_PER_VIDEO = 10  # frames per video for LSTM\n\ndef extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(total_frames // num_frames, 1)\n    \n    for i in range(num_frames):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n        frame = preprocess_input(frame)\n        frames.append(frame)\n    cap.release()\n    \n    # pad if fewer frames\n    while len(frames) < num_frames:\n        frames.append(frames[-1])\n        \n    return np.array(frames)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:30:56.934440Z","iopub.execute_input":"2026-01-04T16:30:56.935057Z","iopub.status.idle":"2026-01-04T16:30:56.940584Z","shell.execute_reply.started":"2026-01-04T16:30:56.935033Z","shell.execute_reply":"2026-01-04T16:30:56.939830Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom tensorflow.keras.utils import to_categorical\n\n# Parameters\nMAX_VIDEOS_PER_CLASS = 20  # safe for Kaggle GPU\nFRAMES_PER_VIDEO = 10\nIMG_SIZE = 224\nDATASET_PATH = \"/kaggle/input/ucf101-videos\"\nVIDEOS_FOLDER = f\"{DATASET_PATH}/train\"  # videos inside train/\nCSV_FILE = f\"{DATASET_PATH}/train.csv\"\n\n# ----------------- Safe extract_frames -----------------\ndef extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    \n    if not cap.isOpened():\n        print(\"Cannot open video:\", video_path)\n        return np.zeros((num_frames, IMG_SIZE, IMG_SIZE, 3))\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if total_frames == 0:\n        cap.release()\n        print(\"Empty video:\", video_path)\n        return np.zeros((num_frames, IMG_SIZE, IMG_SIZE, 3))\n    \n    step = max(total_frames // num_frames, 1)\n    \n    for i in range(num_frames):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n        frame = preprocess_input(frame)\n        frames.append(frame)\n    \n    cap.release()\n    \n    # pad frames if fewer than num_frames\n    while len(frames) < num_frames:\n        frames.append(frames[-1] if frames else np.zeros((IMG_SIZE, IMG_SIZE, 3)))\n    \n    return np.array(frames)\n\n# ----------------- Load CSV -----------------\n# Read CSV safely (some Kaggle CSVs may not have headers)\ndf = pd.read_csv(CSV_FILE, header=None, names=['video', 'label'])\nprint(\"First rows of CSV:\\n\", df.head())\n\n# Limit videos per class\ndf = df.groupby('label').head(MAX_VIDEOS_PER_CLASS)\n\n# Build label map\nlabels_unique = df['label'].unique()\nlabel_map = {label: idx for idx, label in enumerate(labels_unique)}\nNUM_CLASSES = len(label_map)\n\n# ----------------- Build X, y -----------------\nX, y = [], []\n\nfor idx, row in df.iterrows():\n    vid_path = os.path.join(VIDEOS_FOLDER, row['video'])\n    \n    if not os.path.exists(vid_path):\n        print(\"Missing video:\", vid_path)\n        frames = np.zeros((FRAMES_PER_VIDEO, IMG_SIZE, IMG_SIZE, 3))\n    else:\n        frames = extract_frames(vid_path, num_frames=FRAMES_PER_VIDEO)\n    \n    X.append(frames)\n    y.append(label_map[row['label']])\n\nX = np.array(X)  # shape: (num_videos, FRAMES_PER_VIDEO, 224,224,3)\ny = to_categorical(y, NUM_CLASSES)\n\nprint(\"Dataset shape:\", X.shape, y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:38:08.616790Z","iopub.execute_input":"2026-01-04T16:38:08.617581Z","iopub.status.idle":"2026-01-04T16:38:17.623856Z","shell.execute_reply.started":"2026-01-04T16:38:08.617523Z","shell.execute_reply":"2026-01-04T16:38:17.623118Z"}},"outputs":[{"name":"stdout","text":"First rows of CSV:\n                        video        label\n0                 video_name          tag\n1  v_CricketShot_g08_c01.avi  CricketShot\n2  v_CricketShot_g08_c02.avi  CricketShot\n3  v_CricketShot_g08_c03.avi  CricketShot\n4  v_CricketShot_g08_c04.avi  CricketShot\nMissing video: /kaggle/input/ucf101-videos/train/video_name\nDataset shape: (101, 10, 224, 224, 3) (101, 6)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:42:02.787761Z","iopub.execute_input":"2026-01-04T16:42:02.788725Z","iopub.status.idle":"2026-01-04T16:42:03.268296Z","shell.execute_reply.started":"2026-01-04T16:42:02.788692Z","shell.execute_reply":"2026-01-04T16:42:03.267434Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Augmentation for frames\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    brightness_range=[0.8,1.2]\n)\n\nval_datagen = ImageDataGenerator()  # no augmentation for validation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:42:17.865286Z","iopub.execute_input":"2026-01-04T16:42:17.865862Z","iopub.status.idle":"2026-01-04T16:42:17.869891Z","shell.execute_reply.started":"2026-01-04T16:42:17.865834Z","shell.execute_reply":"2026-01-04T16:42:17.869081Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def sequence_generator(X, y, batch_size=4, augmentor=None):\n    n = len(X)\n    while True:\n        idx = np.random.permutation(n)\n        for i in range(0, n, batch_size):\n            batch_idx = idx[i:i+batch_size]\n            batch_X = []\n            for vid in X[batch_idx]:\n                frames = []\n                for frame in vid:\n                    if augmentor:\n                        frame = augmentor.random_transform(frame)\n                    frames.append(frame)\n                batch_X.append(np.array(frames))\n            yield np.array(batch_X), y[batch_idx]\n\n# Generators\ntrain_gen = sequence_generator(X_train, y_train, augmentor=train_datagen)\nval_gen = sequence_generator(X_val, y_val, augmentor=None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:42:31.377482Z","iopub.execute_input":"2026-01-04T16:42:31.378223Z","iopub.status.idle":"2026-01-04T16:42:31.383311Z","shell.execute_reply.started":"2026-01-04T16:42:31.378197Z","shell.execute_reply":"2026-01-04T16:42:31.382614Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\ncnn_base = MobileNetV2(\n    weights='imagenet',\n    include_top=False,\n    pooling='avg',\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\n\n# Freeze all except last 15 layers\nfor layer in cnn_base.layers[:-15]:\n    layer.trainable = False\nfor layer in cnn_base.layers[-15:]:\n    layer.trainable = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:42:53.018267Z","iopub.execute_input":"2026-01-04T16:42:53.018583Z","iopub.status.idle":"2026-01-04T16:42:54.828240Z","shell.execute_reply.started":"2026-01-04T16:42:53.018528Z","shell.execute_reply":"2026-01-04T16:42:54.827666Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1767544973.188284    2092 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\n\ninputs = Input(shape=(FRAMES_PER_VIDEO, IMG_SIZE, IMG_SIZE, 3))\nx = TimeDistributed(cnn_base)(inputs)  # Apply CNN to each frame\nx = LSTM(128)(x)\nx = Dropout(0.5)(x)\noutputs = Dense(NUM_CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs, outputs)\nmodel.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:42:59.173367Z","iopub.execute_input":"2026-01-04T16:42:59.173688Z","iopub.status.idle":"2026-01-04T16:42:59.330727Z","shell.execute_reply.started":"2026-01-04T16:42:59.173663Z","shell.execute_reply":"2026-01-04T16:42:59.329972Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │     \u001b[38;5;34m2,257,984\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m721,408\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,408</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,980,166\u001b[0m (11.37 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,980,166</span> (11.37 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,762,182\u001b[0m (6.72 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,762,182</span> (6.72 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,217,984\u001b[0m (4.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217,984</span> (4.65 MB)\n</pre>\n"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"steps_per_epoch = len(X_train) // 4\nval_steps = len(X_val) // 4\n\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    epochs=30,  # adjust as needed\n    validation_data=val_gen,\n    validation_steps=val_steps\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:43:09.343821Z","iopub.execute_input":"2026-01-04T16:43:09.344149Z","iopub.status.idle":"2026-01-04T16:49:07.953238Z","shell.execute_reply.started":"2026-01-04T16:43:09.344122Z","shell.execute_reply":"2026-01-04T16:49:07.952568Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767545034.893135    2661 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 0.1607 - loss: 1.8670 - val_accuracy: 0.0833 - val_loss: 1.7599\nEpoch 2/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 570ms/step - accuracy: 0.1062 - loss: 1.7789 - val_accuracy: 0.3333 - val_loss: 1.5421\nEpoch 3/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 605ms/step - accuracy: 0.2336 - loss: 1.6227 - val_accuracy: 0.2500 - val_loss: 1.4610\nEpoch 4/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598ms/step - accuracy: 0.3820 - loss: 1.4054 - val_accuracy: 0.4167 - val_loss: 1.3139\nEpoch 5/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 592ms/step - accuracy: 0.3614 - loss: 1.4304 - val_accuracy: 0.5833 - val_loss: 1.1971\nEpoch 6/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 595ms/step - accuracy: 0.4638 - loss: 1.3018 - val_accuracy: 0.8333 - val_loss: 1.0840\nEpoch 7/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 594ms/step - accuracy: 0.6407 - loss: 1.2003 - val_accuracy: 1.0000 - val_loss: 0.9623\nEpoch 8/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 598ms/step - accuracy: 0.6013 - loss: 1.1803 - val_accuracy: 1.0000 - val_loss: 0.9040\nEpoch 9/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 591ms/step - accuracy: 0.7591 - loss: 0.9673 - val_accuracy: 0.9167 - val_loss: 0.7977\nEpoch 10/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 596ms/step - accuracy: 0.7710 - loss: 0.8724 - val_accuracy: 1.0000 - val_loss: 0.6919\nEpoch 11/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 582ms/step - accuracy: 0.6700 - loss: 1.0580 - val_accuracy: 0.9167 - val_loss: 0.7221\nEpoch 12/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 583ms/step - accuracy: 0.7018 - loss: 0.8693 - val_accuracy: 0.9167 - val_loss: 0.6126\nEpoch 13/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 592ms/step - accuracy: 0.7837 - loss: 0.7608 - val_accuracy: 0.9167 - val_loss: 0.6309\nEpoch 14/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 581ms/step - accuracy: 0.9775 - loss: 0.5259 - val_accuracy: 1.0000 - val_loss: 0.4675\nEpoch 15/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 580ms/step - accuracy: 0.9708 - loss: 0.5509 - val_accuracy: 0.9167 - val_loss: 0.4857\nEpoch 16/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 596ms/step - accuracy: 0.8988 - loss: 0.5941 - val_accuracy: 0.9167 - val_loss: 0.4415\nEpoch 17/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 596ms/step - accuracy: 0.8660 - loss: 0.5926 - val_accuracy: 1.0000 - val_loss: 0.4029\nEpoch 18/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 580ms/step - accuracy: 0.9831 - loss: 0.4461 - val_accuracy: 1.0000 - val_loss: 0.3237\nEpoch 19/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 603ms/step - accuracy: 0.9207 - loss: 0.4462 - val_accuracy: 1.0000 - val_loss: 0.3026\nEpoch 20/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 605ms/step - accuracy: 0.9256 - loss: 0.4969 - val_accuracy: 1.0000 - val_loss: 0.2870\nEpoch 21/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 586ms/step - accuracy: 0.9731 - loss: 0.3821 - val_accuracy: 1.0000 - val_loss: 0.2605\nEpoch 22/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 603ms/step - accuracy: 0.9861 - loss: 0.2956 - val_accuracy: 1.0000 - val_loss: 0.2277\nEpoch 23/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 593ms/step - accuracy: 0.8911 - loss: 0.3369 - val_accuracy: 1.0000 - val_loss: 0.2518\nEpoch 24/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 600ms/step - accuracy: 0.9337 - loss: 0.3628 - val_accuracy: 1.0000 - val_loss: 0.2179\nEpoch 25/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 599ms/step - accuracy: 0.9849 - loss: 0.2755 - val_accuracy: 1.0000 - val_loss: 0.2315\nEpoch 26/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 590ms/step - accuracy: 0.9773 - loss: 0.3982 - val_accuracy: 1.0000 - val_loss: 0.2185\nEpoch 27/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 589ms/step - accuracy: 0.9639 - loss: 0.3354 - val_accuracy: 1.0000 - val_loss: 0.1676\nEpoch 28/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 590ms/step - accuracy: 0.9692 - loss: 0.2078 - val_accuracy: 1.0000 - val_loss: 0.1240\nEpoch 29/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 587ms/step - accuracy: 0.9799 - loss: 0.2456 - val_accuracy: 1.0000 - val_loss: 0.1659\nEpoch 30/30\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 593ms/step - accuracy: 0.9769 - loss: 0.2147 - val_accuracy: 1.0000 - val_loss: 0.1508\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"loss, acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:49:12.450089Z","iopub.execute_input":"2026-01-04T16:49:12.450395Z","iopub.status.idle":"2026-01-04T16:49:25.051888Z","shell.execute_reply.started":"2026-01-04T16:49:12.450368Z","shell.execute_reply":"2026-01-04T16:49:25.051129Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - accuracy: 0.9375 - loss: 0.2400\nTest Accuracy: 0.9375\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Save the entire model to a single HDF5 file\nMODEL_SAVE_PATH = \"/kaggle/working/ucf101_mobilenet_lstm.h5\"\nmodel.save(MODEL_SAVE_PATH)\n\nprint(\"Model saved to:\", MODEL_SAVE_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:49:33.432750Z","iopub.execute_input":"2026-01-04T16:49:33.433486Z","iopub.status.idle":"2026-01-04T16:49:33.750113Z","shell.execute_reply.started":"2026-01-04T16:49:33.433459Z","shell.execute_reply":"2026-01-04T16:49:33.749407Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"Model saved to: /kaggle/working/ucf101_mobilenet_lstm.h5\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load HDF5 model\nMODEL_PATH = \"/kaggle/working/ucf101_mobilenet_lstm.h5\"\nmodel = load_model(MODEL_PATH)\nprint(\"Model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:50:35.599274Z","iopub.execute_input":"2026-01-04T16:50:35.600072Z","iopub.status.idle":"2026-01-04T16:50:36.333927Z","shell.execute_reply.started":"2026-01-04T16:50:35.600037Z","shell.execute_reply":"2026-01-04T16:50:36.333168Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"video_path = \"/kaggle/input/ucf101-videos/test/v_Punch_g02_c01.avi\"\n\n# Extract frames\nframes = extract_frames(video_path, num_frames=FRAMES_PER_VIDEO)\n\n# Add batch dimension\nframes = np.expand_dims(frames, axis=0)  # shape: (1, FRAMES_PER_VIDEO, 224,224,3)\n\n# Predict\npred = model.predict(frames)\npred_class_idx = np.argmax(pred, axis=1)[0]\npred_label = list(label_map.keys())[list(label_map.values()).index(pred_class_idx)]\n\nprint(\"Predicted action:\", pred_label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T16:53:27.749314Z","iopub.execute_input":"2026-01-04T16:53:27.750054Z","iopub.status.idle":"2026-01-04T16:53:27.923521Z","shell.execute_reply.started":"2026-01-04T16:53:27.750025Z","shell.execute_reply":"2026-01-04T16:53:27.922793Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\nPredicted action: Punch\n","output_type":"stream"}],"execution_count":25}]}